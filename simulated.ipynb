{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining ten topic-word distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0 = np.array([ 100,100,100,100,100, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0 ])\n",
    "topic_1 = np.array([ 100,0,0,0,0, 100,0,0,0,0, 100,0,0,0,0, 100,0,0,0,0, 100,0,0,0,0 ])\n",
    "topic_2 = np.array([ 0,0,0,0,100, 0,0,0,0,100, 0,0,0,0,100, 0,0,0,0,100, 0,0,0,0,100 ])\n",
    "topic_3 = np.array([ 0,100,0,0,0, 0,100,0,0,0, 0,100,0,0,0, 0,100,0,0,0, 0,100,0,0,0 ])\n",
    "topic_4 = np.array([ 100,0,0,0,0, 0,100,0,0,0, 0,0,100,0,0, 0,0,0,100,0, 0,0,0,0,100 ])\n",
    "topic_5 = np.array([ 100,100,100,100,100, 0,0,0,0,100, 0,0,0,0,100, 0,0,0,0,100, 0,0,0,0,100 ])\n",
    "topic_6 = np.array([ 0,0,0,0,0, 100,100,100,100,100, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0 ])\n",
    "topic_7 = np.array([ 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,100,100, 0,0,0,100,100 ])\n",
    "topic_8 = np.array([ 100,100,0,0,0, 100,100,0,0,0, 100,100,0,0,0, 0,0,0,0,0, 0,0,0,0,0 ])\n",
    "topic_9 = np.array([ 0,0,0,0,0, 0,0,0,0,0, 0,0,100,100,100, 0,0,100,100,100, 0,0,100,100,100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling these topic word distributions so they are actually probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_dist = topic_0.reshape(1,-1) / np.sum(topic_0)\n",
    "t1_dist = topic_1.reshape(1,-1) / np.sum(topic_1)\n",
    "t2_dist = topic_2.reshape(1,-1) / np.sum(topic_2)\n",
    "t3_dist = topic_3.reshape(1,-1) / np.sum(topic_3)\n",
    "t4_dist = topic_4.reshape(1,-1) / np.sum(topic_4)\n",
    "t5_dist = topic_5.reshape(1,-1) / np.sum(topic_5)\n",
    "t6_dist = topic_6.reshape(1,-1) / np.sum(topic_6)\n",
    "t7_dist = topic_7.reshape(1,-1) / np.sum(topic_7)\n",
    "t8_dist = topic_8.reshape(1,-1) / np.sum(topic_8)\n",
    "t9_dist = topic_9.reshape(1,-1) / np.sum(topic_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdists = [t0_dist, t1_dist, t2_dist, t3_dist, t4_dist, t5_dist, t6_dist, t7_dist, t8_dist, t9_dist]\n",
    "tdists = [np.squeeze(item) for item in tdists]\n",
    "num_topics = len(tdists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the number of documents (images in this case) and the number of (not necessarily unique) words in each document. The dimension just relates to the size of the image. Each pixel in an image is a possible word, so there are $dim^2$ words in the vocabulary of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000\n",
    "num_words = 100\n",
    "dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the topic-word distributions. For example, in the right most topic, words (pixels) come from only the top row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAA+CAYAAAC2oBgNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB40lEQVR4nO3cwU3DMBiGYbdiCMSdO0sgJmBKJkBdgjt3xBQxExBbKCSOv+e5tgKnTqRXlvJfaq21AACxrkcvAAA4lhgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAINxd7xeX78f/XMemrvefv37Wcx0vD08brubvbsvb6ufP19edVrLu/euj+Z21Pem5jtb/2GvPWnsyy/01y/NeyjjPSY+1PZnlOkrZ5v4a4ZmfaU+cDABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAITrnjMwyrvRPW7L0SsgVc9zMsrMhNHXUErfDIs9/gbMzskAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAuO45A8A2Wu/w97wXP8ocgDM4029lRkqeUeZgOBkAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJyhQzCYniE5rUElZxq0w1j2GILj/hyPkwEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBw5gzACbXe097jXXFgHk4GACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAh3qbXWoxcBABzHyQAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIT7ATFJTaKflgrGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "fig, axes = plt.subplots(1,len(tdists))\n",
    "for i, t in enumerate(tdists):\n",
    "    axes[i].imshow(tdists[i].reshape(dim, dim))\n",
    "    axes[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I generate synthetic data by, for each document (image), I pick a distribution over topics by drawing from a multinomial distribution. The probabilities associated with each topic form the multinomial topic distribution. I then draw from the multinomial distribution over topics $num_words$ times which gives the frequencies of how many words originating from some topic appear in the document. Then, for each topic, I draw the corresponding number of words for that topic from that topic's topic-word distribution. After this process, I have $num_images \\times num_words$ words and their corresponding assignments stored in `w_corp` and `z_corp` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corp = []\n",
    "z_corp = []\n",
    "\n",
    "for iter in range(num_images):\n",
    "    dir_draw = np.random.dirichlet([1] * num_topics, 1)[0]\n",
    "    mult_draw = np.random.multinomial(num_words, pvals = dir_draw)\n",
    "    w_i = np.zeros(num_words)\n",
    "    z_i = np.zeros(num_words)\n",
    "    i = 0\n",
    "    for t, count in enumerate(mult_draw):\n",
    "        words = np.random.choice(dim * dim, size=count, p=tdists[t])\n",
    "        for j, word in enumerate(words):\n",
    "            w_i[i] = words[j]\n",
    "            z_i[i] = t\n",
    "            i += 1 \n",
    "    w_i, z_i = shuffle(w_i, z_i, random_state=0)\n",
    "    w_corp.append(w_i)\n",
    "    z_corp.append(z_i)\n",
    "w_corp = np.array(w_corp)\n",
    "z_corp = np.array(z_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an idea of what the synthetic data looks like, I draw 50 \"documents\" and show what they look like. The goal is to then recover the topic-word distributions used to generate these, only from the synthetic data. At least to my eye, it is not obvious what the topic-word distributions used to generate these are, only based on the examples shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = 5\n",
    "cols = 10\n",
    "\n",
    "indices = np.random.choice(w_corp.shape[0], rows * cols, replace=False)\n",
    "\n",
    "indices = indices.reshape(rows, cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols)\n",
    "\n",
    "for i, row in enumerate(indices):\n",
    "    images = w_corp[row]\n",
    "    maxfreq = 0\n",
    "    for j, image in enumerate(images):\n",
    "        unique, counts = np.unique(image, return_counts=True)\n",
    "        maxfreq = max(maxfreq, max(counts))\n",
    "    for j, image in enumerate(images):\n",
    "        unique, counts = np.unique(image, return_counts=True)\n",
    "        freqs = np.zeros(dim * dim)\n",
    "        for num, ct in zip(unique, counts):\n",
    "            freqs[int(num)] = ct\n",
    "        axes[i,j].imshow(freqs.reshape(dim, dim), vmin=0, vmax=maxfreq)\n",
    "        axes[i,j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the procedure, I generate an initial state of the Markov chain, which initially assigns topics to words randomly. There are $num\\_images \\times num\\_words$ topic assignments in this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_assignments = np.random.randint(0,num_topics,size=num_images * num_words)\n",
    "len(topic_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = topic_assignments.reshape(num_images, num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters of the underlying Dirichlet distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two data structures allow me to track $n_j^{(w)}$ and $n_j^{(d)}$ without having to recalculate every time. Using the trick mentioned in Griffiths and Steyvers, one need only increment the entries according to the reassigned word, which is much less costly than recalculating $n_j^{(w)}$ and $n_j^{(d)}$ every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_data = np.zeros((num_images, num_topics))\n",
    "word_data = np.zeros((dim * dim, num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I populate the two data structures above for the initial random topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(num_images):\n",
    "    for w in range(num_words):\n",
    "        document_data[d, topic_assignments[d,w]] += 1\n",
    "        word_data[int(w_corp[d,w]), topic_assignments[d,w]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function draws from the full conditional distribution on topic assignments, or $p(z_i |\\mathbf{z}_{-i}, \\mathbf{w})$, as derived in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_draw_fcd(state, ind):\n",
    "    d = math.floor(ind / num_words)\n",
    "    w_ind = ind - d * num_words\n",
    "    word = int(w_corp[d, w_ind])\n",
    "    topic = state[d, w_ind]\n",
    "\n",
    "    doc_topic = document_data[d].copy()\n",
    "    doc_topic[topic] -= 1\n",
    "    nddot = np.sum(doc_topic)\n",
    "    doc_topic_dist = (doc_topic + alpha) / (nddot + num_topics * alpha)\n",
    "\n",
    "    topic_word = word_data[word].copy()\n",
    "    topic_word[topic] -= 1\n",
    "\n",
    "    njdot = np.sum(document_data, axis=0)\n",
    "    topic_word_dist = (topic_word + beta) / (njdot + dim * dim * beta)\n",
    "\n",
    "    fcd = doc_topic_dist * topic_word_dist\n",
    "\n",
    "    fcd_rescale = fcd / np.sum(fcd)\n",
    "    return np.random.choice(num_topics, 1, p=fcd_rescale)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function corresponds to one iteration of gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_iter(w_corp, state):\n",
    "    for i in range(num_words * num_images):\n",
    "        d = math.floor(i / num_words)\n",
    "        w_ind = i - d * num_words\n",
    "        old_topic = state[d,w_ind].copy()\n",
    "        new_state_i = new_draw_fcd(state, i)\n",
    "        word = int(w_corp[d,w_ind])\n",
    "        state[d,w_ind] = new_state_i\n",
    "        \n",
    "        document_data[d, old_topic] -=1\n",
    "        document_data[d, new_state_i] +=1\n",
    "\n",
    "        word_data[word, old_topic] -=1\n",
    "        word_data[word, new_state_i] += 1\n",
    "\n",
    "    return state.reshape(num_images, num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I run Gibbs sampling for 1000 iterations, starting from the initially random topic assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "state = topic_assignments\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    state = gibbs_iter(w_corp, topic_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates an estimate of each topic-word distribution $\\phi_j$ for a topic $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phi(words, z_hat, t):\n",
    "    tprobs = np.zeros(dim * dim)\n",
    "    words_in_topic = words.reshape(1,-1)[z_hat.reshape(1,-1) == t]\n",
    "    unique, counts = np.unique(words_in_topic, return_counts=True)\n",
    "    njdot = np.sum(counts)\n",
    "    for word in range(dim * dim):\n",
    "        numerator = beta\n",
    "        denom = dim * dim * beta + njdot\n",
    "        if word in unique:\n",
    "            numerator += counts[np.where(unique == word)][0]\n",
    "        tprobs[word] = numerator / denom\n",
    "    return tprobs.reshape(dim, dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the state vector after 1000 iterations of gibbs sampling, I calculate $\\phi_j$ for each topic $j$. It appears that this does a pretty good job at recovering the initial topic-word distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, num_topics)\n",
    "\n",
    "for t in range(num_topics):\n",
    "     ax[t].imshow(calc_phi(w_corp, state, t))\n",
    "     ax[t].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
